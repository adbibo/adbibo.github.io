{"meta":{"title":"Lester","subtitle":"只要今天努力，明天会比昨天更美好","description":"君子所取者远，则必有所待，所就者大，则必有所忍，志大而量广，才有余而识具足，则必取其远，必就其大！！","author":"Lester","url":"http://yoursite.com"},"pages":[{"title":"comment","date":"2016-07-07T13:43:11.000Z","updated":"2022-06-21T13:16:03.192Z","comments":true,"path":"comment/index.html","permalink":"http://yoursite.com/comment/index.html","excerpt":"","keywords":null,"text":"你好，一起来讨论下感兴趣的事情","raw":null,"content":null},{"title":"categories","date":"2016-11-11T08:00:09.000Z","updated":"2016-11-11T08:06:42.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","keywords":null,"text":"","raw":null,"content":null},{"title":"","date":"2021-09-03T02:40:14.402Z","updated":"2021-09-03T01:56:29.930Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","keywords":null,"text":"关于作者 从事数据开发多年, 涉猎多个行业, 现在想静下心来总结下这些年的一些工作方法论, 写一点笔记","raw":null,"content":null},{"title":"tags","date":"2016-11-11T07:54:59.000Z","updated":"2016-11-11T08:08:50.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","keywords":null,"text":"","raw":null,"content":null}],"posts":[{"title":"大厂是怎样做数据治理的?","slug":"大厂是怎样做数据治理的","date":"2023-06-21T12:39:33.000Z","updated":"2024-02-18T12:00:31.619Z","comments":true,"path":"2023/06/21/大厂是怎样做数据治理的/","link":"","permalink":"http://yoursite.com/2023/06/21/%E5%A4%A7%E5%8E%82%E6%98%AF%E6%80%8E%E6%A0%B7%E5%81%9A%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%E7%9A%84/","excerpt":"","keywords":null,"text":"数据治理，这几年很火的一个话题，为什么要做数据治理？这可能是很多数据治理项目负责人需要给他的老板回答的问题。 “因为在数据产生、采集、加工、存储、应用到销毁的全过程中，每个环节都可能会引入各种质量、效率或安全相关的问题。在公司早期的发展阶段，这些数据问题对公司发展的影响并不是很大，公司对问题的容忍度相对也比较高。但是，随着业务的发展，公司在利用数据资产创造价值的同时，对数据质量和稳定性要求也有所提升。此外，当数据积累得越来越多，公司对数据精细化运营程度的要求也随之提高，会逐渐发现有很多问题需要治理。” 最近阅读了几篇大厂做数据治理的一些文章, 其中有国内大厂美团酒旅事业部,有网易严选, 也有国外大厂Uber做数据治理的一些经验。 美团酒旅需要治理哪些问题?数据治理是一项需要长期被关注的复杂工程，这项工程通过建立一个满足企业需求的数据决策体系，在数据资产管理过程中行使权力、管控和决策等活动，并涉及到组织、流程、管理制度和技术体系等多个方面。一般而言，数据治理的治理内容主要包括下面几个部分： 质量问题：这是最重要的问题，很多公司的数据部门启动数据治理的大背景就是数据质量存在问题，比如数仓的及时性、准确性、规范性，以及数据应用指标的逻辑一致性问题等。 成本问题：互联网行业数据膨胀速度非常快，大型互联网公司在大数据基础设施上的成本投入占比非常高，而且随着数据量的增加，成本也将继续攀升。 效率问题：在数据开发和数据管理过程中都会遇到一些影响效率的问题，很多时候是靠“盲目”地堆人力在做。 安全问题：业务部门特别关注用户数据，一旦泄露，对业务的影响非常之大，甚至能左右整个业务的生死。 标准问题：当公司业务部门比较多的时候，各业务部门、开发团队的数据标准不一致，数据打通和整合过程中都会出现很多问题。 关键行动 数据治理策略 确定数据治理的内容: 组织、标准规范、技术、衡量指标 整体数据治理的实现路径是以标准化的规范和组织保障为前提，通过做技术体系整体保证数据治理策略的实现。同时，搭建数据治理的衡量体系，随时观测和监控数据治理的效果，保障数据治理长期向好的方向发展。 标准化和组织保障 制定全链路标准, 从数据采集、数仓开发、指标管理到数据生命周期管理 联合组建业务部门的数据管理委员会-组织保障 技术系统 核心是系统替代人工 数据质量保障 统一数仓规范建模 OneModel – 工具保障-数仓规范化开发系统-Dataman 统一指标逻辑管理 OneLogic 统一数据服务OneService 统一用户产品入口 OnePortal – 将数据门户按照使用用户和应用场景划分为3类, 决策分析/业务数据查询/数据资产信息查询 提升数据运营效率 RD人工答疑–&gt;答疑机器人 衡量指标 网易严选需要治理哪些问题? 模型 希望在模型设计开发方面有更多的规范、制度、知识沉淀。 任务运维 希望任务能及时、准确、稳定产出，同时出现问题可以快速定位问题、影响评估和辅助解决问题。 报警优化 减少晚上值班由于任务出错或异常报警次数，并有一些干预措施。 链路感知 任务血缘（任务之间的依赖）复杂，希望有一些链路感知能力（某一任务变更对上下游任务有提示及流程管控能力），此链路感知目前用于减少资损事件发生场景。 测试卡点 在建设之前严选数仓还没有相应的测试环境，也没有测试辅助功能来协助数据开发进行测试，希望有测试环境及相关辅助测试功能，同时QA同学可以对核心或重点任务进行测试卡点及审核，从而提升任务和数据的质量。 重大事故快速恢复 严选一些核心任务或源头数据出现问题后，在项目建设之前恢复可能需要1~2天左右时间，严重影响了数据的正常使用，希望可以从技术层面来协助快速解决，从而提升用户体验。 在后续还是有很多事情要做的，如下： 关键链路诊断：目前是单链路或输入任务，后续可以考虑多链路排序展示，开发人员来选择多条链路分析。 多基线任务问题联合定位：多条基线任务智能定位，发现单任务引起的多基线多报警问题，做智能取消处理。 影响分析进阶：指标层面影响分析，多任务影响联合分析。 报警配置优化：值班组报警配置向基线报警配置优化、报警接收方式、报警类型、报警次数拆分等。 数据质量评估体系：重点推进模型（复用、健壮、规范）、任务、指标等质量评估指标体系的建设。 关键行为 事前—模型上线前第一道防线 流程保障 需求 需求管理,背景/指标口径/价值/deadline 研发 需求评审/模型设计/测试 生产 数据质量稽核配置和任务运维 模型设计保障 dwd 面向业务过程设计, 不需要需求输入 dws 面向需求设计 dm 面向需求设计 维度提炼 数据质量保障 完整性/唯一性/有效性/一致性/精确性 测试保障 测试中心– 测试环境 核心功能: 数据对比: 分区, 字段, 从模型和字段两个粒度给出相关一致性统计指标, 可查看不一致明细信息 形态探查: 对模型的构成信息进行探查, 数量/字段长度/空值占比/最大最小值/枚举值 链路感知保障 末级模型打标 根据模型血缘对上游模型整体打标签 上有模型对应任务变更提交时会有相应提示活流程管控 事中—基于基线任务运维 任务运维中心 及时发现问题&gt;快速定位问题&gt;全面影响评估&gt;辅助解决问题 通过不同基线对任务机型划分, 从而可以针对不同基线任务采取不同的策略动作 基线划分原则 基线只针对天调度任务 核心应用 核心指标 预留破线buffer 统一基线 2:30基线 dwd 4:30基线 dws 7:30基线 核心产品应用 9:30基线 数仓全部主题域、指标、应用模型产出 数仓默认基线： 不重要的任务可以挂在默认基线上，若出现集群资源紧张情况，则可以对此基线任务进行查杀 报警过多措施 电话报警智能取消 真多单任务有多个报警，如已电话报警，则20分钟内报警取消 报警时间间隔调整 会减少报警次数， 但也增加了破线未提前干预风险 重复报警整合 关键链路诊断 事后—报警干预措施及常态化机制 制定干预措施 问题处理知识库 关键链路诊断&amp;影响分析 重大事故快速恢复 常态化事项 冷任务定级 耗时任务优化 引擎切换或升级 任务长链路优化 维表设计优化 任务报警兜底优化 监控复盘 Uber需要治理哪些问题?对于一些重要的数据问题，我们没有给予足够的关注，在规模扩大之后，它们变得更加重要，涉及的具体问题包括： 数据重复： 一些关键数据和指标缺少一个真实的数据来源，这导致了重复、不一致，并且在使用时会有很多困惑。消费者必须从解决业务问题中抽出时间来做大量的尽职调查，从而弥补这一点。使用自助服务工具创建的数十万个数据集加剧了这个问题，因为我们无法明显看出哪个数据集更重要。 发现问题： 如果没有丰富的元数据和分面搜索，在数十万数据集中发现数据是很困难的。糟糕的发现导致了重复的数据集、重复的工作和不一致的答案（这取决于回答问题时所使用的数据）。 工具互不连通： 数据流经许多工具、系统和组织。但是我们的工具没有相互集成，导致工作重复和糟糕的开发体验——例如，必须在多个工具之间复制粘贴文档和所有者信息；开发者无法自信地修改数据模式，因为不清楚它在下游是如何使用的。 日志不一致： 在移动设备上的日志是手动完成的；日志没有统一的结构，我们无法通过简单、一致的方法度量用户的实际行为，只能通过推断来判定（这低效且容易出错）。 流程缺失： 缺乏跨团队的数据工程流程，导致各个团队的成熟度不同，团队间没有一致的数据质量定义或指标。 所有权和 SLA 缺失： 数据集没有明确的属主——它们通常没有质量保证、错误修复的 SLA 不一致，电话支持、事件管理与我们对服务的管理方式相去甚远。 关键行动 数据即代码 Data As Code, 模型的复用及扩展要优于创建新模型 数据要有Owner, 每份数据都必须有一个明确的owner和明确的使用场景, 并且在用完后废弃掉 数据质量要已知, data quality is known, 加速数据生产效率, 数据产品要 总结 事前, 制定合理的规范,明确健全的流程 事中, 优化要到位, 任务优化, 链路优化 事后, 总结复盘, 知识库 Reference 美团酒旅数据治理实践 网易严选数仓任务治理实践 数据赋能：Uber的数据治理实践分享","raw":null,"content":null,"categories":[{"name":"数据治理","slug":"数据治理","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86/"}],"tags":[{"name":"数据治理","slug":"数据治理","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86/"}]},{"title":"手把手教你如何搭建单机spark+iceberg环境","slug":"手把手教你如何搭建单机spark-iceberg环境","date":"2022-04-23T08:24:38.000Z","updated":"2022-06-28T08:28:50.065Z","comments":true,"path":"2022/04/23/手把手教你如何搭建单机spark-iceberg环境/","link":"","permalink":"http://yoursite.com/2022/04/23/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E5%8D%95%E6%9C%BAspark-iceberg%E7%8E%AF%E5%A2%83/","excerpt":"","keywords":null,"text":"背景:最近周末闲来无事, 用自己的mac参照网上的一些教程搭了一套环境 hadoop3.3.1+hive3.1.2+spark3.1.2+iceberg0.13.1 1. hadoop环境brew 安装 hadoop 安装命令 1$ brew install hadoop 安装结果 查看安装目录 1$ brew list hadoop 配置Hadoop相关文件（此处伪分布式，还有单机模式和完全分布式模式） 修改/opt/homebrew/Cellar/hadoop/3.3.1/libexec/etc/hadoop/hadoop-env.sh，添加内容如下： 1export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home 修改/opt/homebrew/Cellar/hadoop/3.3.1/libexec/etc/hadoop/core-site.xml，添加如下内容： 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/homebrew/Cellar/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.lester.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.lester.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注： fs.default.name 保存了NameNode的位置，HDFS和MapReduce组件都需要用到它，这就是它出现在core-site.xml 文件中而不是 hdfs-site.xml文件中的原因。在该处配置HDFS的地址和端口号。 修改/opt/homebrew/Cellar/hadoop/3.3.1/libexec/etc/hadoop/mapred-site.xml，添加内容如下： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 变量mapred.job.tracker 保存了JobTracker的位置，因为只有MapReduce组件需要知道这个位置，所以它出现在mapred-site.xml文件中。 修改/opt/homebrew/Cellar/hadoop/3.3.1/libexec/etc/hadoop/hdfs-site.xml，添加内容如下： 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 变量dfs.replication指定了每个HDFS默认备份方式通常为3, 由于我们只有一台主机和一个伪分布式模式的DataNode，将此值修改为1。 配置完毕，运行hadoop 跳转目录cd /opt/homebrew/Cellar/hadoop/3.3.1/bin/ 启动hadoop之前需要格式化hadoop系统的HDFS文件系统 1$ hadoop namenode -format 接着进入cd /opt/homebrew/Cellar/hadoop/3.3.1/sbin/，执行 1$ ./start-all.sh 或者分开启动: 12$ ./start-dfs.sh$ ./start-yarn.sh 警告：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 这对Hadoop的运行没有影响 jps命令查看java进程 2. hive配置1234$ $HADOOP_HOME/bin/hadoop fs -mkdir /tmp$ $HADOOP_HOME/bin/hadoop fs -mkdir /user/hive/warehouse$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse 创建元数据库Hive默认用derby作为元数据库。这里我们用mysql来存储元数据，下面作一些初始化 1234567mysql&gt; create database metastore;mysql&gt; create user &#x27;hive&#x27;@&#x27;localhost&#x27; identified by &#x27;yinhc&#x27;;mysql&gt; grant select,insert,update,delete,alter,create,index,references on metastore.* to &#x27;hive&#x27;@&#x27;localhost&#x27;;mysql&gt; flush privileges; 下载hive, https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/ 解压 12cp hive-default.xml.template hive-site.xmlvim hive-site.xml # 调整一些关键参数值 123456789101112131415161718192021222324252627282930313233343536373839&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/metastore&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive(填上述mysql中创建的用户名)&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;yinhc&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt;&lt;/property&gt; 拷贝mysql-connector到hive给Hive的lib目录下拷贝一个mysql-connector 123$ curl -L &#x27;http://www.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.42.tar.gz/from/http://mysql.he.net/&#x27; | tar xz$ cp mysql-connector-java-5.1.42/mysql-connector-java-5.1.42-bin.jar /usr/local/Cellar/hive/2.1.1/libexec/lib/ 初始化库初始化一下metastore库 1$ schematool -initSchema -dbType mysql 3. spark环境4. iceberg0.13启动: start-hadoop start-spark start-hive-ms 1234567891011121314151617alias start-hadoop=&#x27;sh /opt/homebrew/Cellar/hadoop/3.3.1/sbin/start-all.sh&#x27;alias stop-hadoop=&#x27;sh /opt/homebrew/Cellar/hadoop/3.3.1/sbin/stop-all.sh&#x27;alias start-spark-tf=&#x27;sh /Users/lester/opt/spark/sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.bind.host=localhost --hiveconf hive.server2.thrift.port=10000&#x27;alias start-spark=&#x27;sh /Users/lester/opt/spark/sbin/start-all.sh&#x27;alias stop-spark=&#x27;sh /Users/lester/opt/spark/sbin/stop-all.sh&#x27;alias start-hive-ms=&#x27;nohup hive --service metastore &gt;&gt; /Users/lester/opt/hive/log/metastore.log &amp;&#x27;alias start-hs2=&#x27;cd /Users/lester/opt/hive &amp;&amp; nohup bin/hiveserver2 &gt;&gt; /Users/lester/opt/hive/log/hiveserver2.log &amp;&#x27;alias beeline=&#x27;/Users/lester/opt/spark/bin/beeline -u jdbc:hive2://localhost:10000/default -n lester -p lester&#x27;alias start-iceberg=&#x27;start-hadoop &amp;&amp; start-hive-ms &amp;&amp; start-spark&#x27;alias stop-iceberg=&#x27;stop-spark &amp;&amp; stop-hadoop&#x27; URL: Resource Manager: http://locaclhost:9870 JobTracker: http://localhost:8088 Specific Node Information: http://localhost:8042 http://localhost:8080","raw":null,"content":null,"categories":[],"tags":[]},{"title":"程序员之路-shell篇","slug":"程序员之路-shell篇","date":"2017-12-12T09:53:55.000Z","updated":"2022-06-21T13:09:37.883Z","comments":true,"path":"2017/12/12/程序员之路-shell篇/","link":"","permalink":"http://yoursite.com/2017/12/12/%E7%A8%8B%E5%BA%8F%E5%91%98%E4%B9%8B%E8%B7%AF-shell%E7%AF%87/","excerpt":"","keywords":null,"text":"查看命令帮助文档1.help Command 适用于内部命令, type.举例： $ type cd cd is a shell builtin $ help cd $ type ls ls is aliased to `ls --color=auto&apos; $ help ls -bash: help: no help topics match `ls&apos;. Try `help help&apos; or `man -k ls&apos; or `info ls&apos;. 2. Command –help/-h 适用于外部命令例如： $ ls --help 3. man Command 举例： $ man cd 文本展示cat cat是显示文件夹的命令，这个大家都知道， tac tac是cat的倒写，意思也和它是相反的。cat是从第一行显示到最后一行，而tac是从最后一行显示到第一行， rev rev 则是从最后一个字符显示到第一个字符 $ cat file asdf sdfa $ tac sdfa asdf $ rev fdsa afds 统计wc -l 文本文件 |文本文件行数 awkawk -F &quot;,&quot; &apos;&#123;print $1&#125;&apos; $file |uniq` #从以“，”作分隔符的文件中读取第一列，并输出不重复的项 cat $file |awk -F &quot;,&quot; &apos;&#123;print $1&#125;&apos; $file|uniq` Linux下给普通用户添加Sudo权限 一般在Linux下创建用户后不能直接使用sudo命令，所以需要在/etc/sudoers配置中添加用户的 权限。 首先使用有sudo权限的账户登录系统，然后运行 # sudo nano /etc/sudoers ，找到 root ALL=(ALL) ALL 后，在下一行添加 UserName ALL=(ALL) ALL 即可。使用CTRL+指定字母操作 ssh免密登录 平时工作难免会使用到多台服务器，从一台服务器ssh登录另一台服务器就输入密码，太影响效率。 通过以下方法可以免密登录。 在A机下生成公钥/私钥对。 $ssh-keygen -t rsa -P 把A机下的id_rsa.pub复制到B机下，在B机的.ssh/authorized_keys文件里，我用scp复制。 $scp .ssh/id_rsa.pub user@host:/path/to/user/.ssh/id_rsa.pub 由于还没有免密码登录的，所以要输入密码。 B机把从A机复制的id_rsa.pub添加到.ssh/authorzied_keys文件里。 $cat .ssh/id_rsa.pub&gt;&gt;.ssh/authorized_keys chmod 600 .ssh/authorized_keys authorized_keys的权限要是600。 端口 netstat -anp 查看端口使用情况 lsof -i:$PORT 查看使用该端口($PORT)的程序 cat /etc/services 也可以查看端口使用情况 chkconfig 查看系统服务的开启状态 去除linux系统文本中的^M 对于回车符的定义： windows：0D0A unixlinux: 0A MAC: 0D 比较快捷的去除这些符号的方法有这么几种： 1. vim： 使用vim打开文本文件 vi dos.txt ##命令模式下输入 :set fileformat=unix :w ## VI下使用正则表达式替换 :g/^M/s/^M// ## 或者 :%s/^M//g 2. sedsed ’s/^M//’ filename &gt; tmp_filename 3. trtr -d &quot; &quot; 4. dos2unixdos2unix filename 时间计算以下参考链接 计算5分钟前的时间ymdhmis=`perl -e &quot;print sprintf &apos;%04d-%02d-%02d %02d:%02d:%02d&apos;,(localtime(time()-300))[5]+1900,(localtime(time()-300))[4]+1,(localtime(time()-300))[3],(localtime(time()-300))[2],(localtime(time()-300))[1],(localtime(time()-300))[0]&quot;` echo $ymdhmis 计算5分钟前的日期ymd=`perl -e &quot;print sprintf &apos;%04d-%02d-%02d&apos;,(localtime(time()-300))[5]+1900,(localtime(time()-300))[4]+1,(localtime(time()-300))[3]&quot;` echo $ymd","raw":null,"content":null,"categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://yoursite.com/tags/Shell/"}]},{"title":"简单数据开发流程","slug":"简单数据开发流程","date":"2017-12-01T15:06:25.000Z","updated":"2017-12-01T15:51:48.000Z","comments":true,"path":"2017/12/01/简单数据开发流程/","link":"","permalink":"http://yoursite.com/2017/12/01/%E7%AE%80%E5%8D%95%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B/","excerpt":"","keywords":null,"text":"最近给组内同事做了一个小分享,先讲下背景.部门属于新组建,主要业务是为各事业部开发一些BI报表.本次分享主要涉及与产品经理的沟通和数据开发过程中的细节, 本人平时开发过程中负责数据开发和后端接口开发. 1. 需求沟通1.1 产品原型确定 时效,首先确定数据时效 1,临时需求 要求响应时间短 2,长期需求 慢工出细活 权限 权限划分,安全问题 产品原型细节 1.2 数据准备 数据来源 存储方式 excel 文本文件 关系型数据库 nosql 接口 数据更新方式 1, 日、周、月、季 2, 实时 1.3 需求分析 结合需求和实际数据，参照经验，分析，并给出反馈 2. 数据开发2.1 数据逻辑整理 数据更新方式 1, 日、周、月、季 2, 实时 数据维度整理 1, 维度标准化 产品经理给出映射表 自己设计 2, 结合实际数据，考虑要比产品经理更长远 2.2 数据开发 数据库配置 测试库 预发布库 正式库 数据库设计 数据库表 索引 数据维度 结合实际数据，考虑要比产品经理更长远 数据更新方式 日、周、月、季 部署 环境 权限 数据库权限 2.3 数据展示后台 域名 测试域名 预发布域名 正式域名 接口 参数设计 依据产品原型设计 默认参数 接口内容格式 与前端配合，给出最适合前端处理的格式 接口文档 每个字都要让前端看的懂 权限 权限分级 权限不分级部署方式 环境 3 结果验证 数据验证 1.维度验证 结果是否涵盖了数据实际全维度 实际数据没有覆盖到的维度，是否设置了默认值 数据结果指标验证 此处需要产品经理或数据分析人员配合 后台验证 接口验证 权限验证 上线验证 结果交付 以上是本次分享的所有大纲,具体的细节应依据开发过程中遇到的实际问题而定.","raw":null,"content":null,"categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"}]},{"title":"Django项目单机部署","slug":"Django项目单机部署","date":"2017-10-09T06:39:51.000Z","updated":"2022-06-21T13:14:44.693Z","comments":true,"path":"2017/10/09/Django项目单机部署/","link":"","permalink":"http://yoursite.com/2017/10/09/Django%E9%A1%B9%E7%9B%AE%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2/","excerpt":"","keywords":null,"text":"1, Django项目开发以app的形式添加项目中的各个功能模块，将静态文件保存在项目根目录下，static存储css、js、image等，templates存储html。 2，在项目部署服务器安装ngnix、uwsgi此处教程较多，不在此做特别说明。 3，配置uwsgi在django项目根目录下新建文件uwsgi.ini，其中${project_name}为项目名称。 # uwsgi ini file [uwsgi] # http://stackoverflow.com/questions/14194859/importerror-no-module-named-django-core-wsgi-for-uwsgi project_name = $&#123;project_name&#125; # Django-related settings # the base directory (full path) chdir = %d # the absolute path of the directory containing the configuration file # http://uwsgi-docs.readthedocs.io/en/latest/Configuration.html # Django&apos;s wsgi file module = %(project_name).wsgi # %n the filename without extension # process-related settings # master master = true # maximum number of worker processes # adjust according to http://uwsgi-docs.readthedocs.io/en/latest/ThingsToKnow.html processes = 2 max-requests = 200 # the socket (use the full path to be safe socket = /tmp/%(project_name).sock # ... with appropriate permissions - may be needed chmod-socket = 666 #chown-socket = www-data:www-data # clear environment on exit vacuum = true daemonize = /tmp/%(project_name).log pidfile = /tmp/%(project_name).pid # added 2014-08-25 #emperor = /etc/uwsgi/vassals #uid = www-data #gid = www-data # added 2014-09-17 reload-on-as = 126 reload-on-rss enable-threads = true pythonpath = %d # the absolute path of the directory containing the configuration file env = LANG=en_US.UTF-8 # http://stackoverflow.com/questions/10396141/strange-unicodeencodeerror-using-os-path-exists 4，配置nginx在django项目根目录下新建project_nginx.conf，其中${project_name}为项目名称, ${domain_name}为项目域名。 # nginx.conf # the upstream component nginx needs to connect to upstream $&#123;project_name&#125;_project &#123; server unix:/tmp/$&#123;project_name&#125;.sock; # for a file socket &#125; # configuration of the server server &#123; # the port your site will be served on listen 80; # the domain name it will serve for server_name $&#123;domain_name&#125;; # substitute your machine&apos;s IP address or FQDN, use one of `listen` or `server_name` charset utf-8; # max upload size client_max_body_size 200M; # adjust to taste # |css|js was removed for develop purpose # 添加这个缓存机制会导致部分图片404 #location ~* .(woff|eot|ttf|svg|mp4|webm|jpg|jpeg|png|gif|ico)$ &#123; # expires 365d; #&#125; location /favicon.ico &#123; root /home/www/$&#123;project_name&#125;/tmp/images; &#125; # Django media location /media &#123; alias /home/www/$&#123;project_name&#125;/tmp; # your Django project&apos;s media files - amend as required &#125; location /static &#123; alias /home/www/$&#123;project_name&#125;/tmp; # your Django project&apos;s static files - amend as required &#125; # Finally, send all non-media requests to the Django server. location / &#123; uwsgi_pass $&#123;project_name&#125;_project; include /etc/nginx/uwsgi_params; # the uwsgi_params file you installed # include /usr/local/etc/nginx/uwsgi_params; &#125; &#125; 5，编写部署脚本在django项目根目录下新建文件run_server.sh，其中${project_name}为项目名称。 #!/usr/bin/env bash # 将项目目录下的Nginx配置文件链接至Nginx目录下的conf.d目录下 ln -s /home/www/$&#123;project_name&#125;/$&#123;project_name&#125;_nginx.conf /etc/nginx/conf.d/ # 选择python版本 source ~/.bash_profile pyenv local 2.7.12 # 安装项目依赖包 cd /home/www/$&#123;project_name&#125; pip install -r requirements.txt # 收集django静态文件 python /home/www/$&#123;project_name&#125;/manage.py collectstatic --noinput # 针对首次部署之后的部署，杀死已有进程，重新部署 kill -9 `cat /tmp/$&#123;project_name&#125;.pid` uwsgi --ini /home/www/$&#123;project_name&#125;/uwsgi.ini # 修改Nginx配置后重新加载生效 sudo /etc/init.d/nginx reload","raw":null,"content":null,"categories":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"django","slug":"django","permalink":"http://yoursite.com/tags/django/"}]},{"title":"MySQL数据库优化","slug":"数据库优化","date":"2017-09-16T04:17:26.000Z","updated":"2022-06-21T13:12:38.950Z","comments":true,"path":"2017/09/16/数据库优化/","link":"","permalink":"http://yoursite.com/2017/09/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96/","excerpt":"","keywords":null,"text":"本文大部分内容摘自简书网 大牛的经验之谈：数据库查询速度优化技巧及解决方案， 后续会结合自己在工作中的经历做一些针对性的案例分析。 1，尽可能减少表的全局扫描 减少where字段值null判断 尽可能地将可能为null的字段设置一个可区分的默认值 尽量避免在where子句中使用!= 或&lt;&gt;操作符 尽量避免在where子句中使用or来连接条件 尽量避免使用in和not in 2，不要在条件判断时进行算数运算所以不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算,这样系统将可能无法正确使用索引。 3，利用exists代替in4，索引技巧并不是所有索引对查询都有效 SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 索引要有区分度。 索引并不是越多越好索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 应尽可能的避免更新 clustered 索引数据列因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 尽量使用数字型字段若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 5，创建数据库时应该注意地方尽可能的使用 varchar/nvarchar 代替 char/nchar因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 用表变量来代替临时表。 如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 避免频繁创建和删除临时表，以减少系统表资源的消耗。 6，尽量避免使用游标 因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 7，数据放回时注意什么 尽量避免大事务操作，提高系统并发能力。这样可以有效提高系统的并发能力 尽量避免向客户端返回大数据量。若数据量过大，应该考虑相应需求是否合理。","raw":null,"content":null,"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"后端","slug":"后端","permalink":"http://yoursite.com/tags/%E5%90%8E%E7%AB%AF/"}]},{"title":"Python后端开发——大纲","slug":"Python后端开发——大纲","date":"2017-09-10T00:47:53.000Z","updated":"2022-06-21T13:13:32.455Z","comments":true,"path":"2017/09/10/Python后端开发——大纲/","link":"","permalink":"http://yoursite.com/2017/09/10/Python%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E2%80%94%E2%80%94%E5%A4%A7%E7%BA%B2/","excerpt":"","keywords":null,"text":"最近准备整理工作中用到的一些知识，先列一个大纲，作为这个系列的目录，争取每周至少一更。 Python基础语法常用数据结构 list dict set，frozenset tuple 常用语法 列表推导式 字典推导式 集合推导式 迭代器 生成器 装饰器 常用内置库 re os sys string collections itertools json datetime hashlib pickle ConfigParser 常用第三方库 requests beautifulsoup SQLalchmey pymysql 框架djangoflasktornadotwistedgeventRestful API数据库 MySQL Redis Mongodb Memcache PostgreSQL 优化","raw":null,"content":null,"categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"数据开发规范-补充中","slug":"数据开发规范-补充中","date":"2017-09-08T09:52:18.000Z","updated":"2017-09-08T10:22:44.000Z","comments":true,"path":"2017/09/08/数据开发规范-补充中/","link":"","permalink":"http://yoursite.com/2017/09/08/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83-%E8%A1%A5%E5%85%85%E4%B8%AD/","excerpt":"","keywords":null,"text":"数据库相关字段命名字段命名应该结合业务需求，应注意如下： 可读性，通过名称就能知晓该字段意思，但不宜过长； 从长期的接口开发工作总结知晓，数据分为以下几种： 数值型：此类数据，不考虑小数点后数据 小数型：此类数据，需考虑小数点后保留位数 百分数型：此类数据命名时应加为rate或ratio作为标识，与前端约定此类数据均按百分比展示，即末尾添加‘%’符号 趋势数据一般按照时间字段来展示，所以在建表时，应将最终需要展示的时间维度数据以展示结果格式来创建，若该字段无法准确排序，应添加时间维度排序编号 维度数据不宜过长，且应添加排序编号 索引创建，应将所有维度都创建索引，方便在接口操作的时候快速定位数据。 数据结果普查很多时候，产品经理在画原型图时没有接触实际的业务数据，无法根据实际情况考虑最终展示结果，所以数据开发人员在处理完结果数据并入库后应查看结果数据表，并对数据做排查，review所有的情况，做到心中有数。建议从以下方面来排查： 维度数据齐全，与最终展示相符； 维度映射关系明确，做到一对一映射； 所有数值合理，以下范例可供参考： 百分数范围在(0,1)，如若遇特殊情况超出，应做说明； 司龄数值合理，之前有发现该数据存在260，明显是不合理，应排查处理逻辑； 数据接口开发因为数据组一般接口开发都采用django框架，所以本文档仅针对数据组内部情况说明，分为API形式和非API形式。 API形式此处不做过多说明，直接调用rest_framework框架处理即可。 非API形式非API形式，主要是为了前端开发人员节省时间。以非API形式提供接口，主要是方便在多个筛选的情况下快速开发接口。 从最终展示来看，数据接口大概分为以下几种： 表格：此类数据可处理为header + content的形式 header：列出表头展示内容和对应content中的数据字段名； content：按照表头排序输出列表 趋势图：维度信息和数值内容 维度信息：一般是多维度。其中时间排序即可。 数值信息：一般为数据，定义统一处理函数对数据进行格式化。 柱形图：与趋势图类似 联动选项：主要是针对维度筛选，此处定义多层字典即可。","raw":null,"content":null,"categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"}]},{"title":"mysql数据库权限操作","slug":"mysql数据库权限操作","date":"2016-12-06T09:05:31.000Z","updated":"2022-06-21T13:13:47.034Z","comments":true,"path":"2016/12/06/mysql数据库权限操作/","link":"","permalink":"http://yoursite.com/2016/12/06/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9D%83%E9%99%90%E6%93%8D%E4%BD%9C/","excerpt":"","keywords":null,"text":"设置用户权限以root账号登录mysql mysql -hlocalhost -uroot -p mysql&gt; use mysql; 创建用户create user mike identified by &apos;mike&apos;; create user rainbow@localhost identified by &apos;rainbow&apos;; 修改用户名rename user mike to john; 删除用户drop user rainbow; 查看用户select host,user from user; 更改密码update mysql.user set authentication_string=password(&apos;123qwe&apos;) where user=&apos;john&apos;; alter user &apos;john&apos;@&apos;localhost&apos; identified by &apos;123qwe&apos;; 给用户添加权限常规权限： select，查 update，改 delete，删 insert，增 数据库开发人员权限： create, 建表 alter, 修改 drop, 删除 references, 操作外键权限 create tempoary tables, 操作临时表权限 index, 操作索引权限 create view, 操作试图权限 show view, 查看试图源代码权限 create routine, 操作存储过程权限 alter routine, execute, 操作mysq函数权限 DBA权限： grant all on *.* to dba@&apos;localhost&apos; #高级DBA管理MySQL中所有数据库权限 设置权限时必须给出一下信息 要授予的权限 被授予访问权限的数据库或表 用户名 grant和revoke可以在几个层次上控制访问权限 整个服务器，使用 grant ALL 和revoke ALL 整个数据库，使用on database.* 特点表，使用on database.table 特定的列 特定的存储过程 user表中host列的值的意义 % 匹配所有主机 localhost localhost不会被解析成IP地址，直接通过UNIXsocket连接 127.0.0.1 会通过TCP/IP协议连接，并且只能在本机访问 ::1 ::1就是兼容支持ipv6的，表示同ipv4的127.0.0.1 1.为用户adbibo添加数据库test中所有表的查询权限 grant select on test.* to &apos;adbibo&apos;@&apos;localhost&apos; identified by &apos;adbibo&apos;; 2.为用户adbibo添加数据库test中所有表的增删改权限 grant update,delete,insert on test.* to &apos;adbibo&apos;@&apos;localhost&apos; identified by &apos;adbibo&apos;; 3.为用户adbibo添加数据库中所有的权限，最高权限 grant all privileges on *.* to &apos;adbibo&apos;@&apos;localhost&apos; identified by &apos;adbibo&apos;; 4.删除用户adbibo插入数据库的权限 revoke insert on *.* from &apos;adbibo&apos;@&apos;localhost&apos;; 附： 权限表 权限 说明 all alter 修改表 create 创建表 drop 删除表 index 可以使用create index 和drop index create view 创建试图 show view 显示视图内容 select 查询表 update 更新表内容 delete 删除表内容 insert 向表中插入记录 grant option 可以使用grant create user 创建用户 show databases 显示当前用户可访问的所有数据库名 alter routine 使用alter procedure 和drop procedure create routine 使用create procedure create temporary tables 使用create temporary table execute 使用call和存储过程 file 使用select into outfile 和load data infile和revoke lock tables 锁表 process 使用show full processlist reload 使用flush replication client 服务器位置访问 replocation slave 由复制从属使用 shutdown 使用mysqladmin shutdown 来关闭mysql super usage 无访问权限","raw":null,"content":null,"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"GIT使用实战","slug":"GIT使用实战","date":"2016-11-11T07:48:49.000Z","updated":"2022-06-21T13:14:21.975Z","comments":true,"path":"2016/11/11/GIT使用实战/","link":"","permalink":"http://yoursite.com/2016/11/11/GIT%E4%BD%BF%E7%94%A8%E5%AE%9E%E6%88%98/","excerpt":"","keywords":null,"text":"参考教程推荐廖雪峰的Git教程 省去账号申请等信息，以我自己使用过程遇到的问题和解决的方式来给大家讲讲。 问题一：实际coding时，我有github平台和gitlab平台的需求，如何在本机配置？一般.ssh都在用户目录下， # adbibo @ bogon in ~/.ssh [15:04:49] $ pwd /Users/adbibo/.ssh 生成一个新的ssh key $ ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; Generating public/private rsa key pair. Enter file in which to save the key (/Users/adbibo/.ssh/id_rsa):/Users/adbibo/.ssh/test_id_rsa #在此输入你要保存ssh key的实际文件名，为了区分github和gitlab，请将ssh key存储在不同的文件中。 Enter passphrase (empty for no passphrase): 输入密码，鄙人比较懒，直接回车跳过 Enter same passphrase again:同上 Your identification has been saved in /Users/adbibo/.ssh/test_id_rsa. Your public key has been saved in /Users/adbibo/.ssh/test_id_rsa.pub. The key fingerprint is: SHA256:xRAuPVjXDjJUKihSqQWb9TwBMH3Be4ChaMxm8C+3j24 your_email@example.com The key&apos;s randomart image is: +---[RSA 4096]----+ |=+=*o. .=oo. | |=O=o+o =o=. . | |*Oo.=o+ =ooo | |=. o...o o . | | . o. S | | o . | | . | | Eo | | oo . | +----[SHA256]-----+ 生成的ssh key分别将验证信息和公钥存储在test_id_rsa和test_id_rsa.pub文件中。 在.ssh目录下创建文件config $ cd ~/.ssh $ touch config 添加如下内容： # gitlab Host gitlab.com HostName gitlab.com IdentityFile ~/.ssh/gitlab_rsa # github Host github.com HostName github.com IdentityFile ~/.ssh/github_rsa 至此，gitlab和github的项目通过git命令管理了。 问题二：如果将本地的工程提交到远程仓库？这里以github为例， 首先在github.com上创建工程，点击”New repository”，输入Repository name，填写Description，选择项目是public还是private，然后点击”Create repository”。得到一个如下形式的git仓库名。 git@github.com:yourgithubname/test.git 然后回到本机，在工程目录下执行如下命令： git init # 在本地初始化git仓库 git add . # 添加当前目录下的所有文件作为需要版本管理的文件 git commit -m &quot;第一次提交&quot; # 填写提交信息 git remote add origin git@github.com:yourgithubname/test.git # 关联远程仓库 git push -u origin master # 将本地文件提交到远程仓库 一般都会顺利地进行至此，但是我在github.com上创建工程时，选择了生成.gitignore文件，于是远程仓库初始化时就有了文件。再在本机进行上面的操作就会报错了。 To github.com:yourgithubname/test.git ! [rejected] master -&gt; master (fetch first) error: failed to push some refs to &apos;git@github.com:yourgithubname/test.git&apos; hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., &apos;git pull ...&apos;) before pushing again. hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 提示需要先git pull，就是先要将远程仓库的文件pull到本机，但是由于本机使用IDE自动生成了.gitignore文件，与远程仓库发生了冲突。需要带上参数–allow-unrelated-histories $ git pull origin master --allow-unrelated-histories From github.com:yourgithubname/test * branch master -&gt; FETCH_HEAD Auto-merging README.md CONFLICT (add/add): Merge conflict in README.md Auto-merging .gitignore CONFLICT (add/add): Merge conflict in .gitignore Automatic merge failed; fix conflicts and then commit the result. 此时又提示merge conflict， 所以需要手动做更改.gitignore和README.md文件，具体根据需求进行更改。然后重新执行如下命令： $git add . $git commit -m &apos;commot message&apos; $ git push 显示如下信息就说明commit成功了。 Counting objects: 17, done. Delta compression using up to 4 threads. Compressing objects: 100% (15/15), done. Writing objects: 100% (17/17), 4.15 KiB | 0 bytes/s, done. Total 17 (delta 2), reused 0 (delta 0) 问题三：遇到再补充。常用git仓库 github.com 开源项目很多，可以管理个人创建的项目，public的项目免费，如果创建private的需要money。可以搭个人blog，我用的hexo，简单配置即可。 gitlab.com 也不错，private项目免费，看中的就这点，毕竟github用的很习惯。 对于不喜欢命令行的朋友，推荐使用sourceTree这个工具来管理， 附上链接sourceTree","raw":null,"content":null,"categories":[{"name":"GIT","slug":"GIT","permalink":"http://yoursite.com/categories/GIT/"}],"tags":[{"name":"GIT","slug":"GIT","permalink":"http://yoursite.com/tags/GIT/"}]},{"title":"使用Cloudera快速部署Hadoop集群(二)","slug":"使用Cloudera快速部署Hadoop集群-二","date":"2016-09-07T03:36:12.000Z","updated":"2022-06-21T13:11:50.773Z","comments":true,"path":"2016/09/07/使用Cloudera快速部署Hadoop集群-二/","link":"","permalink":"http://yoursite.com/2016/09/07/%E4%BD%BF%E7%94%A8Cloudera%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Hadoop%E9%9B%86%E7%BE%A4-%E4%BA%8C/","excerpt":"","keywords":null,"text":"一,Cloudera Manager 安装后的各个目录 /var/log/cloudera-scm-installer : 安装日志目录。 /var/log/* : 相关日志文件（相关服务的及CM的）。 /usr/share/cmf/ : 程序安装目录。 /usr/lib64/cmf/ : Agent程序代码。 /var/lib/cloudera-scm-server-db/data: 内嵌数据库目录。 /usr/bin/postgres : 内嵌数据库程序。 /etc/cloudera-scm-agent/ : agent的配置目录。 /etc/cloudera-scm-server/ : server的配置目录。 /opt/cloudera/parcels/ : Hadoop相关服务安装目录。 /opt/cloudera/parcel-repo/ : 下载的服务软件包数据，数据格式为parcels。 /opt/cloudera/parcel-cache/ : 下载的服务软件包缓存数据。 /etc/hadoop/* : 客户端配置文件目录。 二,各节点服务详述利用Cloudera Manager安装CDH，需要配置各节点角色，现将各服务节点角色一一说明。 1,HDFSApache hadoop官网描述如下： Hadoop Distributed File System (HDFS™): A distributed file system that provides high-throughput access to application data. Gateway HttpFS NameNode DataNode SecondaryNameNode Balancer HttpFS NFS Gateway 2,HBase Gateway Master HBase REST Server HBase Thrift Server Region Server 3,YRAN Resource Manager JobHistory Server NodeManager 4,Spark History Server Gateway Livy Server 5,ZooKeeper server 6,Hive Gateway Hive Metastore Server WebHcat HiveServer2 7,HUE Hue Server 8,Impala Impala Catalog Server Impala Llama ApplicationMaster Impala Daemon 9,Oozie Oozie Server 10,Solr Gateway Solr Server 11,Key-Value Store Indexer Lily HBase Indexer","raw":null,"content":null,"categories":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/categories/%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/tags/%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91/"},{"name":"CDH","slug":"CDH","permalink":"http://yoursite.com/tags/CDH/"}]},{"title":"使用Cloudera快速部署Hadoop集群(一)","slug":"使用Cloudera快速部署Hadoop集群-一","date":"2016-08-29T14:10:44.000Z","updated":"2016-11-11T08:33:42.000Z","comments":true,"path":"2016/08/29/使用Cloudera快速部署Hadoop集群-一/","link":"","permalink":"http://yoursite.com/2016/08/29/%E4%BD%BF%E7%94%A8Cloudera%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Hadoop%E9%9B%86%E7%BE%A4-%E4%B8%80/","excerpt":"","keywords":null,"text":"换到新公司已经一个半月，做了两个小项目，现在在进行第三个大项目。由于部门是新成立的，需要搭建数据中心的基础数据平台，我先试个水。 之前的工作经历导致对Hadoop生态了解的不多，因为原来公司的数据都在阿里云上。不需要了解细节就可以很好地工作，而那时的我也比较懵逼，所以。。。 好久没有更新blog，准备接下来每周都讲讲自己平台搭建的工作。所有文中提到的参考链接均为查阅资料时的参考。 Cloudera Manager安装1，Cloudera Manager安装包： http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5/RPMS/x86_64/ 2，Cloudera Manager资源http://archive.cloudera.com/cm5/cm/5/ 3，parcelshttp://archive.cloudera.com/cdh5/parcels/5.8/ 4，tarballhttp://archive.cloudera.com/cdh5/repo-as-tarball/5.8.0/ CDH服务配置1，解析Cloudera Manager内部结构、功能包括配置文件、目录位置等http://my.oschina.net/cloudcoder/blog/362374 2，YARN/MRv2 Resource Manager深入剖析—RM总体架构http://dongxicheng.org/mapreduce-nextgen/yarnmrv2-resource-manager-infrastructure/ 3，HDFS详解http://my.oschina.net/crxy/blog/348868 http://blog.chinaunix.net/uid-27105712-id-3274395.html 4，HUEhttp://ju.outofmemory.cn/entry/105162 5，Hive详细教程http://my.oschina.net/yangzhiyuan/blog/228362 6，HBasehttp://www.cnblogs.com/JemBai/archive/2012/07/21/2602432.html","raw":null,"content":null,"categories":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/categories/%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/tags/%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91/"},{"name":"CDH","slug":"CDH","permalink":"http://yoursite.com/tags/CDH/"}]},{"title":"认识ETL","slug":"认识ETL","date":"2016-07-23T13:37:41.000Z","updated":"2016-11-11T08:33:36.000Z","comments":true,"path":"2016/07/23/认识ETL/","link":"","permalink":"http://yoursite.com/2016/07/23/%E8%AE%A4%E8%AF%86ETL/","excerpt":"","keywords":null,"text":"最近两年一直从事数据处理相关的工作，一年前开始接触数据挖掘的工作，第一个项目就是将两个不同数据源的数据导入项目组odps应用。从数据存放的角度，我的工作分为三部分：1，数据下载（数据存放在不同的服务器）；2，数据清洗格式统一（需要与内部的其它数据保持一致的格式）；3，数据同步至odps（开篇提到的数据需要导入至odps应用中）。其中，主要工作在第二步。当时没有ETL的概念，直到最近新入职一家公司的数据部门，花了不到一周的时间重构了一套ETL流程，才意识到自己一直在从事自己并不认为是ETL开发的工作。于是决定趁今天晚上的时间，利用网络上已有的知识系统地学习ETL。 什么是ETL？ETL－百度百科 ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。ETL一词较常用在数据仓库，但其对象并不限于数据仓库。ETL是构建数据仓库的重要一环，用户从数据源抽取出所需的数据，经过数据清洗,最终按照预先定义好的数据仓库模型，将数据加载到数据仓库中去。 以下内容转自MichaelLau的博客 E－extract需要解决的问题包括： a、数据的时间差异性问题 在抽取旧有数据时，要将不同时期的数据定义统一，较早的数据不够完整或不符合新系统的数据规范，一般可以根据规则，在存入中转区的过程中予以更新或补充。 b、数据的平台多样性问题 在抽取旧有数据时，大部分数据都可采用表复制方式直接导入数据中转区集中，再做处理，但有部分数据可能需要转换成文本文件或使用第三方工具如 Informatica等装载入数据中转区。这部分数据主要是与数据中转区数据库平台不一致的数据库数据，或非存储于数据库内的文本、excel等数据。 c 、数据的不稳定性问题 对于重要信息的完整历史变更记录，在抽取时可以根据各时期的历史信息，在抽取需要信息等基本属性的旧有数据时，要与相应时段的信息关联得到真实的历史属性。 d 、数据的依赖性问题 旧有业务系统的数据关联一般已有约束保证，代码表和参照表等数据也比较准确，但仍有少量数据不完整，对这部分数据，需根据地税的需求采取清洗策略，保证数据仓库各事实表和维表之间的关联完整有效。 数据仓库各事实表和维表的初始装载顺序有先后关系，要有一个集中的数据装载任务顺序方案，确保初始数据装载的准确。这可以通过操作系统或第三方工具的任务调度机制来保证。 T-transform需要注意的问题 数据清洗主要是针对源数据库中出现二义性、重 复、不完整、违反业务或逻辑规则等问题的数据数据进行统一的处理，一般包括如：NULL值处理，日期格式转换，数据类型转换等等。在清洗之前需要进行数据 质量分析，以找出存在问题的数据，否则数据清洗将无从谈起。数据装载是通过装载工具或自行编写的SQL程序将抽取、转换后的结果数据加载到目标数据库中。 数据质量问题具体表现在以下几个方面： a、正确性（Accuracy）：数据是否正确的表示了现实或可证实的来源? b、完整性（Integrity）：数据之间的参照完整性是否存在或一致? c、一致性（Consistency）：数据是否被一致的定义或理解? d、完备性（Completeness）：所有需要的数据都存在吗? e、有效性（Validity）：数据是否在企业定义的可接受的范围之内? f、时效性（Timeliness）：数据在需要的时侯是有效的吗? g、可获取性（Accessibility）：数据是否易于获取、易于理解和易于使用? 以下综合说明数据仓库中数据质量要求，包括格式、完整性要求。 a、业务描述统一，对数据模型的不同版本融合、映射为唯一版本。包括： 1、在业务逻辑没有变化的前提下，旧的业务数据映射在新模型上。 2、 遗留系统的人事信息、考核相关信息与业务系统、行政其他模块要一致。 b、信息描述规范、完整。 1、不存在格式违规,数据类型不存在潜在错误。 2 、参照完整性未被破坏,数据不会找不到参照。 3 、不存在交叉系统匹配违规，数据被很好集成,相同的数据存在于多个系统中，数据之间要匹配。 4 、数据在内部一致,同样的纪录字段在同一个表中重复出现，不能有差别。 L-loading 将转换和清洗完的数据按照数据仓库的结构进行数据加载。需要考虑初始数据装载、数据刷新、加载顺序等等问题。 a、针对数据现状，初始导入有这样一些问题需要考虑： 1、如何解决时间差异性？ 2、如何解决平台差异性？ 3、如何适应数据的不稳定性？ 4、如何解决数据依赖性？ b、数据刷新的策略要根据业务需求和应用系统的承受能力和数据情况决定。主要有这样一些问题需要考虑： 1、如何解决时间差异性？ 2、如何适应数据的不稳定性？ 3、如何解决平台差异性？ 4、如何解决数据依赖性？ 5、如何减少对业务系统的影响？ c、不同的刷新任务类型，对业务系统的影响不同，刷新任务有以下种归类特性： 1、刷新频率：实时刷新、每数小时、每日、每周、每月、不定期手动刷新。 2、刷新方式：数据库表复制、文本文件ftp再装载、物化视图、数据库trigger。 3、数据加工方式：简单插入更新、增加计算项字段、多表关联更新、汇总、多表关联汇总计算。 并可针对各种异常情况做处理：回滚，重新装载，断点重新装载等等，还可在任务完成后（或失败后）将日志以Email方式发给数据仓库管理人员。 以上内容属于搬运，后续在项目中学习到的知识，再来进行补充。","raw":null,"content":null,"categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"}]},{"title":"R语言入门-我的HelloWorld","slug":"R语言入门-我的HelloWorld","date":"2016-05-20T06:17:53.000Z","updated":"2022-06-21T13:13:07.258Z","comments":true,"path":"2016/05/20/R语言入门-我的HelloWorld/","link":"","permalink":"http://yoursite.com/2016/05/20/R%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8-%E6%88%91%E7%9A%84HelloWorld/","excerpt":"","keywords":null,"text":"前几天，主管拿到一份数据，让我做一个分城市统计，原本想先对数据做一些清洗，然后在excel中做一些统计分析。但是转念一想，全国300+的城市，展示是个难题，想到前几天看到组里面另外一个同事——就是我之前提到过的追兵同学，他之前做过一个数据展示的界面，还不错，于是要来他的源码，一知半解地边查资料、边咨询，最后做了一个统计结果的展示界面了，也算是我的R语言入门的HelloWorld了。下面开始我将详细介绍实现R语言代码，其中涉及到公司内部的数据、规格的名称、名词全部更替，最终的也不会放上任何展示界面。 1，数据预处理拿到需要处理的数据，首先需要做预处理，这个切记！ 预处理时，需要根据自己的需求--最终展示需要的事项来展开。这里就不赘述了。 2，Ｒ语言展示部分### 先上代码。 #画图 library(ggplot2) library(plotly) #行列处理 library(dplyr) library(tidyr) #页面展示 library(shiny) #设置工作空间 setwd(&apos;/home/user/project-dir/data/&apos;) #读数据 #假设数据有城市（cityname）,数据类型（data_type）,数据等级（data_class）,其他字段（这里忽略）等字段 log_df &lt;- read.table(&apos;data_file&apos;, header = TRUE, sep = &quot;,&quot;, fileEncoding=&quot;utf-8&quot;) #两个文件有关联信息，这部分数据包含城市（cityname）,城市类型（citytype） config_df &lt;- read.table(&apos;config_file&apos;,header = TRUE, sep = &quot;,&quot;, fileEncoding = &quot;utf-8&quot;) #按城市、数据类型、等级聚合，统计 datatype_df &lt;- df_feedback %&gt;% group_by(cityname, data_type, data_class) %&gt;% summarise(plot_dt_cnt = n()) #按城市和数据类型聚合，统计 datatype_sum_df &lt;- df_feedback %&gt;% group_by(cityname, data_type) %&gt;% summarise(plot_dt_cnt_sum=n()) #聚合上述量表，主要是为了拿到各城市各类型的占比统计所需数据 datatype_df_together &lt;- dplyr::inner_join(datatype_df, datatype_sum_df, by=c(&quot;cityname&quot;, &quot;data_type&quot;)) #新增占比统计字段 datatype_df_together &lt;- datatype_df_together %&gt;% mutate(plot_dt_ratio=plot_dt_cnt/plot_dt_cnt_sum) #按城市和data_class计数 data_class_df &lt;- df_feedback %&gt;% group_by(cityname, data_class, data_type) %&gt;% summarise(plot_rc_cnt=n()) #界面 ui &lt;- fluidPage( #大标题 titlePanel(&quot;全国问题反馈数据相关分布&quot;), fluidRow( #小标题 h4(&quot;各种问题分布&quot;), #选项 column(3,selectInput(inputId = &quot;data_type&quot;, label=&quot;请选择需要展示的问题类型&quot;, choices=levels(datatype_df$data_type), multiple = TRUE, selected=&quot;数据类型-1&quot;)), #内容展示 column(9, plotlyOutput(outputId=&quot;plot_datatype_output&quot;)) ), fluidRow( #小标题 h4(&quot;各城市数据等级分布&quot;), #选项 column(3,selectInput(inputId = &quot;cityname&quot;, label=&quot;请选择需要展示的城市&quot;, choices=levels(data_class_df$cityname), multiple = TRUE, selected=&quot;北京市&quot;)), #内容展示 column(9, plotlyOutput(outputId=&quot;plot_cityname_output&quot;)) ), fluidRow( #小标题 h4(&quot;各城市数据等级分布&quot;), #选项1 column(2,selectInput(inputId = &quot;level&quot;, label=&quot;请选择需要的城市级别&quot;, choices=levels(citylevel_df$level), selected=&quot;S级&quot;)), #选项2 column(2,selectInput(inputId = &quot;level_cityname&quot;, label=&quot;请选择需要的城市&quot;, choices=levels(filter(citylevel_df, level==&quot;S级&quot;)[,1]), selected=&quot;北京市&quot;)), #内容展示 column(8, plotlyOutput(outputId=&quot;plot_level_cityname_output&quot;)) ) ) s_citylist &lt;- filter(citylevel_df, level==&quot;S级&quot;) [,1] #服务 server &lt;- function( input, output, session) &#123; #按数据类型统计输出S级城市统计结果 output$plot_datatype_output &lt;- renderPlotly(&#123; p_datatype &lt;- ggplot(datatype_df_together %&gt;% filter(data_type %in% input$data_type, cityname %in% s_citylist)) + geom_bar(aes(x=as.factor(cityname), weight=plot_dt_ratio, fill=as.factor(data_class)), position=&quot;dodge&quot;) + facet_wrap(~data_type) p_datatype &lt;- p_datatype + xlab(&quot;city&quot;) + ylab(&quot;percent&quot;) ggplotly(p_datatype) &#125;) #按城市统计所有的data_class output$plot_cityname_output &lt;- renderPlotly(&#123; p_cityname &lt;- ggplot(data_class_df %&gt;% filter(cityname %in% input$cityname)) + geom_bar(aes(x = as.factor(data_class), weight=plot_rc_cnt, fill= as.factor(data_type)), position = &quot;dodge&quot;) + xlab(&quot;data_class&quot;) + ylab(&quot;CNT&quot;) + facet_wrap(~cityname) ggplotly(p_cityname) &#125;) #选项联动 choiced_citylist &lt;- reactive(&#123; print(&quot;debug&quot;) citylist &lt;- filter(citylevel_df, level == input$level) %&gt;% mutate(cityname=as.character(cityname)) %&gt;% select(cityname) citylist &lt;- as.vector(citylist) updateSelectInput(session, &quot;level_cityname&quot;, choices = citylist) return (citylist) &#125;) #按城市级别选择城市输出统计结果 output$plot_level_cityname_output &lt;- renderPlotly(&#123; tmp &lt;- choiced_citylist() p_level_cityname &lt;- ggplot(data_class_df %&gt;% filter(cityname==input$level_cityname)) + geom_bar(aes(x = as.factor(data_class), weight=plot_rc_cnt, fill= as.factor(data_type)), position = &quot;dodge&quot;) + xlab(&quot;data_class&quot;) + ylab(&quot;CNT&quot;) + facet_wrap(~cityname) ggplotly(p_level_cityname) &#125;) &#125; shinyApp(ui=ui, server=server) 附：需要一台部署了shiny server的服务器。","raw":null,"content":null,"categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/tags/R%E8%AF%AD%E8%A8%80/"}]},{"title":"如何配置fastCGI使nginx支持CGI","slug":"如何配置fastCGI使nginx支持CGI","date":"2016-05-12T02:28:47.000Z","updated":"2022-06-21T13:10:31.298Z","comments":true,"path":"2016/05/12/如何配置fastCGI使nginx支持CGI/","link":"","permalink":"http://yoursite.com/2016/05/12/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEfastCGI%E4%BD%BFnginx%E6%94%AF%E6%8C%81CGI/","excerpt":"","keywords":null,"text":"之前组内有春日学习计划，安排我分享CGI相关的知识。于是就学习起来，整理了如下文档。 1，安装fcgi-spwan下载链接http://download.lighttpd.net/spawn-fcgi/releases-1.6.x/spawn-fcgi-1.6.2.tar.gz 安装：$wget http://download.lighttpd.net/spawn-fcgi/releases-1.6.x/spawn-fcgi-1.6.2.tar.gz $ tar -zxvf spawn-fcgi-1.6.2.tar.gz $ cd spawn-fcgi-1.6.2 $./configure $make $make install /usr/local/bin/ spawn-fcgi 默认路径 $spawn-fcgi –h Usage: spawn-fcgi [options] [-- &lt;fcgiapp&gt; [fcgi app arguments]] spawn-fcgi v1.6.2 (ipv6) - spawns FastCGI processes Options: -f &lt;path&gt; filename of the fcgi-application (ignored if &lt;fcgiapp&gt; is given) -d &lt;directory&gt; chdir to directory before spawning -a &lt;address&gt; bind to IPv4/IPv6 address (defaults to 0.0.0.0) -p &lt;port&gt; bind to TCP-port -s &lt;path&gt; bind to Unix domain socket -M &lt;mode&gt; change Unix domain socket mode -C &lt;children&gt; (PHP only) numbers of childs to spawn (default: not setting the PHP_FCGI_CHILDREN environment variable - PHP defaults to 0) -F &lt;children&gt; number of children to fork (default 1) -P &lt;path&gt; name of PID-file for spawned process (ignored in no-fork mode) -n no fork (for daemontools) -v show version -?, -h show this help (root only) -c &lt;directory&gt; chroot to directory -S create socket before chroot() (default is to create the socket in the chroot) -u &lt;user&gt; change to user-id -g &lt;group&gt; change to group-id (default: primary group of user if -u is given) -U &lt;user&gt; change Unix domain socket owner to user-id -G &lt;group&gt; change Unix domain socket group to group-id 2，安装fcgiwrap下载链接https://github.com/gnosek/fcgiwrap/archive/master.zip 安装：$wget https://github.com/gnosek/fcgiwrap/archive/master.zip $unzip master.zip $cd fcgiwrap-master/ $autoreconf –i $./configure 如果configure失败，一般都会提示”FastCGI libaray is missing”，确实fcgi-devel 缺少 $ wget http://dl.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm $ sudo rpm -Uvh epel-release*rpm $ sudo yum install fcgi-devel –y $ make $ make install /usr/local/sbin/ fcgiwrap 默认路径 Usage: fcgiwrap [OPTION] Invokes CGI scripts as FCGI. fcgiwrap version 1.1.0 Options are: -f Send CGI&apos;s stderr over FastCGI -c &lt;number&gt; Number of processes to prefork -s &lt;socket_url&gt; Socket to bind to (say -s help for help) -h Show this help message and exit -p &lt;path&gt; Restrict execution to this script. (repeated options will be merged) Report bugs to Grzegorz Nosek &lt;root@localdomain.pl&gt;. fcgiwrap home page: &lt;http://nginx.localdomain.pl/wiki/FcgiWrap&gt; 3，nginx安装下载链接http://tengine.taobao.org/download/tengine-2.1.2.tar.gz 安装 $wget http://tengine.taobao.org/download/tengine-2.1.2.tar.gz $ tar -zxvf tengine-2.1.2.tar.gz $ cd tengine-2.1.2 $./configure $make $make install /usr/local/nginx 默认路径 4，编写CGI脚本设置CGI脚本存放目录，这里只以shell为例，为了逼格，将.sh改为了.cgi $mkdir /usr/local/nginx/cgi-bin/ $cd /usr/local/nginx/cgi-bin/ $vim hello.cgi 1 #!/bin/bash 2 echo &quot;Content-Type:text/html&quot; 3 echo &quot;&quot; 4 5 date 6 echo -e &quot;\\nhello world!&quot; $sudo chmod 755 hello.cgi 5，启动命令sudo /usr/local/bin/spawn-fcgi -f /usr/local/sbin/fcgiwrap -a 127.0.0.1 -p 8092 -F 32 -P /usr/local/nginx/fcgiwrap.pid 6，修改nginx配置$vim /usr/local/nginx/conf/nginx.conf server &#123; listen 8090;#设置监听端口 server_name localhost; error_log /usr/local/nginx/logs/cgi_test.log info; #设置日志路径和模式 location /hello.cgi &#123; fastcgi_param SCRIPT_FILENAME /usr/local/nginx/cgi-bin/$fastcgi_script_name; #设置脚本存放目录 fastcgi_index index.cgi; include fastcgi_params; include fastcgi.conf; fastcgi_pass 127.0.0.1:8092; #监听端口 &#125; &#125; 7，nginx重启Nginx启动： /usr/local/nginx/sbin/nginx Nginx主进程号： ps -ef | grep &quot;nginx: master process&quot; | grep -v &quot;grep&quot; | awk -F &apos; &apos; &apos;&#123;print $2&#125;‘ cat /usr/local/nginx/logs/nginx.pid Nginx配置文件： /usr/local/nginx/conf/nginx.conf Nginx配置文件测试： sudo /usr/local/nginx/sbin/nginx –t 使配置生效： kill -HUP `cat /usr/local/nginx/logs/nginx.pid` 重启nginx后，在浏览器输入http://server_ip:port/hello.cgi 8，参考链接CGI相关：http://www.jdon.com/idea/cgi.htm http://www.cnblogs.com/skynet/p/4173450.html http://www.cnblogs.com/liuzhang/p/3929198.html nginx相关：http://developer.51cto.com/art/201004/194472.htm https://segmentfault.com/a/1190000002797601 http://blog.csdn.net/allenlinrui/article/details/19419721 http://blog.163.com/koumm@126/blog/static/9540383720096307529267/ http://zyan.cc/nginx_php_v5/","raw":null,"content":null,"categories":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"后端服务","slug":"后端服务","permalink":"http://yoursite.com/tags/%E5%90%8E%E7%AB%AF%E6%9C%8D%E5%8A%A1/"},{"name":"CGI","slug":"CGI","permalink":"http://yoursite.com/tags/CGI/"}]},{"title":"R语言入门-数据读取","slug":"R语言入门-数据读取","date":"2016-05-11T13:00:49.000Z","updated":"2022-06-21T13:13:12.852Z","comments":true,"path":"2016/05/11/R语言入门-数据读取/","link":"","permalink":"http://yoursite.com/2016/05/11/R%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96/","excerpt":"","keywords":null,"text":"1，首先需要确认工作空间getwd() 2，设置工作空间setwd(&apos;path&apos;) 3，确认当前路径下是否包含所需处理数据文件dir() 4，读取数据read.csv(&apos;csv_filename&apos;) read.table(&apos;filename&apos;,sep=&apos;delimeter&apos;，header=FALSE)#也可以用来读取csv文件，默认没有列名","raw":null,"content":null,"categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/tags/R%E8%AF%AD%E8%A8%80/"}]},{"title":"R语言入门-尝试","slug":"R语言入门-尝试","date":"2016-05-11T07:59:38.000Z","updated":"2022-06-21T13:13:16.682Z","comments":true,"path":"2016/05/11/R语言入门-尝试/","link":"","permalink":"http://yoursite.com/2016/05/11/R%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8-%E5%B0%9D%E8%AF%95/","excerpt":"","keywords":null,"text":"突然对数据分析利器很感兴趣，就找同事追兵请教了一番。今天刚好整理下整个学习流程。 1，数据读取文本read.table() MySQLinstall.packages(&apos;RMySQL&apos;) ODPSRODPS 其他形式的数据读取（再补充）2，数据表现形式data.frame 3, 数据打理tidyr行变列，列变行（reshape2） dplyr分组汇总（plyr） 4，绘图ggplot2类似PS分图层绘图，图片 plotly动态的JavaScript显示，ggplotly（p） 5， 分享，输出RMarkdown输出为文档：pdf,word,html shiny输出交互类网页，接受用户输入（文本框，按钮，选择框等）。可用于定制分析系统。 6， 空间数据存储与可视化shape，leaflet 上述有很多包名的书写大小写有待考证，先上一部分，后续学习中完善。","raw":null,"content":null,"categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/tags/R%E8%AF%AD%E8%A8%80/"}]}]}